{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c599709c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6f037b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, test_size=0.2, random_state=42, return_preprocessor=False, \n",
    "                     has_target=True, preprocessor=None, label_encoder=None):\n",
    "    \"\"\"\n",
    "    Comprehensive preprocessing function for the Adult Census dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Raw dataset\n",
    "    test_size : float, default=0.2\n",
    "        Proportion of dataset to include in test split (only used if has_target=True)\n",
    "    random_state : int, default=42\n",
    "        Random state for reproducibility\n",
    "    return_preprocessor : bool, default=False\n",
    "        Whether to return the fitted preprocessor object\n",
    "    has_target : bool, default=True\n",
    "        Whether the dataset contains the target variable\n",
    "    preprocessor : sklearn object, default=None\n",
    "        Pre-fitted preprocessor to use (for validation data)\n",
    "    label_encoder : sklearn.preprocessing.LabelEncoder, default=None\n",
    "        Pre-fitted label encoder (for validation data with target)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    If has_target=True and test_size > 0:\n",
    "        X_train, X_test, y_train, y_test : torch.Tensors\n",
    "    If has_target=True and test_size=0:\n",
    "        X_processed, y_encoded : torch.Tensors\n",
    "    If has_target=False:\n",
    "        X_processed : torch.Tensor\n",
    "    \n",
    "    Optional returns if return_preprocessor=True:\n",
    "        preprocessor, label_encoder\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Original dataset shape: {df.shape}\")\n",
    "    print(f\"First 5 rows before transformation:\\n{df.head()}\\n\" + \"=\"*50 + \"\\n\")\n",
    "    \n",
    "    # Step 1: Fix column names\n",
    "    df = df.copy()  # Avoid modifying original dataframe\n",
    "    \n",
    "    # Handle unnamed first column\n",
    "    if df.columns[0] == '' or 'Unnamed' in str(df.columns[0]):\n",
    "        df = df.rename(columns={df.columns[0]: 'id'})\n",
    "    \n",
    "    # Clean column names (remove spaces, special characters)\n",
    "    df.columns = df.columns.str.strip().str.replace(' ', '_').str.replace('-', '_')\n",
    "    \n",
    "    # Step 2: Handle missing values representation\n",
    "    missing_indicators = ['?', 'unknown', 'Unknown', '', ' ', 'n/a', 'N/A', 'na', 'NA']\n",
    "    df = df.replace(missing_indicators, np.nan)\n",
    "    \n",
    "    print(\"Missing values by column:\")\n",
    "    missing_summary = df.isnull().sum()\n",
    "    print(missing_summary[missing_summary > 0])\n",
    "    print()\n",
    "    \n",
    "    # Step 3: Data type corrections\n",
    "    # Ensure numeric columns are actually numeric\n",
    "    numeric_columns = ['age', 'fnlwgt', 'education_num', 'capital_gain', 'capital_loss', 'hours_per_week']\n",
    "    for col in numeric_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    # Step 4: Feature Engineering (Basic)\n",
    "    if 'capital_gain' in df.columns and 'capital_loss' in df.columns:\n",
    "        df['capital_net'] = df['capital_gain'] - df['capital_loss']\n",
    "        df['has_capital_gain'] = (df['capital_gain'] > 0).astype(int)\n",
    "        df['has_capital_loss'] = (df['capital_loss'] > 0).astype(int)\n",
    "    \n",
    "    # Age groups\n",
    "    if 'age' in df.columns:\n",
    "        df['age_group'] = pd.cut(df['age'], \n",
    "                                bins=[0, 25, 35, 45, 55, 65, 100], \n",
    "                                labels=['18_25', '26_35', '36_45', '46_55', '56_65', '65_plus'])\n",
    "    \n",
    "    # Work hours categories\n",
    "    if 'hours_per_week' in df.columns:\n",
    "        df['work_hours_category'] = pd.cut(df['hours_per_week'],\n",
    "                                          bins=[0, 20, 40, 60, 100],\n",
    "                                          labels=['part_time', 'full_time', 'overtime', 'excessive'])\n",
    "    \n",
    "    # Step 5: Separate features and target\n",
    "    if has_target:\n",
    "        # Target column is 'income' or last column\n",
    "        target_col = 'income' if 'income' in df.columns else df.columns[-1]\n",
    "        X = df.drop(columns=[target_col, 'id'], errors='ignore')\n",
    "        y = df[target_col]\n",
    "        \n",
    "        # Encode target\n",
    "        if label_encoder is None:\n",
    "            le = LabelEncoder()\n",
    "            y_encoded = le.fit_transform(y)\n",
    "            print(f\"Target encoding: {dict(zip(le.classes_, le.transform(le.classes_)))}\")\n",
    "        else:\n",
    "            y_encoded = label_encoder.transform(y)\n",
    "            le = label_encoder\n",
    "        \n",
    "        print(f\"Target distribution: {np.bincount(y_encoded) / len(y_encoded)}\\n\")\n",
    "    else:\n",
    "        X = df.drop(columns=['id'], errors='ignore')\n",
    "        le = None\n",
    "    \n",
    "    # Step 6: Identify numeric and categorical columns\n",
    "    numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    print(f\"Identified numeric features ({len(numeric_features)}): {numeric_features}\")\n",
    "    print(f\"Identified categorical features ({len(categorical_features)}): {categorical_features}\\n\")\n",
    "    \n",
    "    # Step 7: Create preprocessing pipelines\n",
    "    if preprocessor is None:\n",
    "        # Numeric pipeline: impute with median, then standardize\n",
    "        numeric_transformer = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "        \n",
    "        # Categorical pipeline: impute with most frequent, then one-hot encode\n",
    "        categorical_transformer = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "        ])\n",
    "        \n",
    "        # Combine preprocessing\n",
    "        preprocessor_obj = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', numeric_transformer, numeric_features),\n",
    "                ('cat', categorical_transformer, categorical_features)\n",
    "            ])\n",
    "        \n",
    "        # Fit and transform\n",
    "        X_processed = preprocessor_obj.fit_transform(X)\n",
    "    else:\n",
    "        # Use provided preprocessor\n",
    "        preprocessor_obj = preprocessor\n",
    "        X_processed = preprocessor_obj.transform(X)\n",
    "    \n",
    "    # Step 8: Split data if target exists and test_size > 0\n",
    "    if has_target and test_size > 0:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_processed, y_encoded, test_size=test_size, \n",
    "            random_state=random_state, stratify=y_encoded\n",
    "        )\n",
    "        \n",
    "        print(f\"Train set: {X_train.shape}, Test set: {X_test.shape}\")\n",
    "        print(f\"Train target distribution: {np.bincount(y_train) / len(y_train)}\")\n",
    "        print(f\"Test target distribution: {np.bincount(y_test) / len(y_test)}\\n\")\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        X_train = torch.FloatTensor(X_train)\n",
    "        X_test = torch.FloatTensor(X_test)\n",
    "        y_train = torch.LongTensor(y_train)\n",
    "        y_test = torch.LongTensor(y_test)\n",
    "        \n",
    "        print(f\"Processed shapes - Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "        print(f\"Tensor shapes - Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "        print(f\"Tensor dtypes - X: {X_train.dtype}, y: {y_train.dtype}\\n\")\n",
    "        \n",
    "        print(\"=\" * 50)\n",
    "        print(\"PREPROCESSING COMPLETED SUCCESSFULLY!\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        if return_preprocessor:\n",
    "            return X_train, X_test, y_train, y_test, preprocessor_obj, le\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    elif has_target:\n",
    "        # No split, return all data\n",
    "        X_processed = torch.FloatTensor(X_processed)\n",
    "        y_encoded = torch.LongTensor(y_encoded)\n",
    "        \n",
    "        if return_preprocessor:\n",
    "            return X_processed, y_encoded, preprocessor_obj, le\n",
    "        return X_processed, y_encoded\n",
    "    \n",
    "    else:\n",
    "        # No target\n",
    "        X_processed = torch.FloatTensor(X_processed)\n",
    "        \n",
    "        if return_preprocessor:\n",
    "            return X_processed, preprocessor_obj\n",
    "        return X_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c08b580",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPTuner:\n",
    "    \"\"\"\n",
    "    MLP Hyperparameter Tuner with Grid Search and Early Stopping\n",
    "    \"\"\"\n",
    "    def __init__(self, X_train, X_test, y_train, y_test, use_cv=True, n_folds=5):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_train, X_test, y_train, y_test : torch.Tensors\n",
    "            Training and test data\n",
    "        use_cv : bool, default=True\n",
    "            Whether to use cross-validation for hyperparameter tuning\n",
    "        n_folds : int, default=5\n",
    "            Number of cross-validation folds\n",
    "        \"\"\"\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "        self.input_size = X_train.shape[1]\n",
    "        self.output_size = len(torch.unique(y_train))\n",
    "        self.use_cv = use_cv\n",
    "        self.n_folds = n_folds\n",
    "        self.results = []\n",
    "    \n",
    "    def create_model(self, hidden_layers, dropout_rate):\n",
    "        \"\"\"Create MLP with specified architecture\"\"\"\n",
    "        layers = []\n",
    "        prev_size = self.input_size\n",
    "        \n",
    "        for hidden_size in hidden_layers:\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            if dropout_rate > 0:\n",
    "                layers.append(nn.Dropout(dropout_rate))\n",
    "            prev_size = hidden_size\n",
    "        \n",
    "        layers.append(nn.Linear(prev_size, self.output_size))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def train_single_fold(self, model, train_loader, val_X, val_y, \n",
    "                         learning_rate, epochs, weight_decay,\n",
    "                         patience=5, min_delta=0.0001):\n",
    "        \"\"\"\n",
    "        Train model on a single fold with early stopping\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        patience : int, default=5\n",
    "            Number of epochs to wait for improvement before stopping\n",
    "        min_delta : float, default=0.0001\n",
    "            Minimum change in validation accuracy to qualify as improvement\n",
    "        \"\"\"\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        \n",
    "        # Early stopping variables\n",
    "        best_val_acc = 0.0\n",
    "        patience_counter = 0\n",
    "        best_model_state = None\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            for batch_X, batch_y in train_loader:\n",
    "                outputs = model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            # Evaluate on validation set\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_outputs = model(val_X)\n",
    "                val_preds = torch.argmax(val_outputs, dim=1)\n",
    "                val_acc = (val_preds == val_y).float().mean().item()\n",
    "                val_loss = criterion(val_outputs, val_y).item()\n",
    "            \n",
    "            # Early stopping check\n",
    "            if val_acc > best_val_acc + min_delta:\n",
    "                best_val_acc = val_acc\n",
    "                patience_counter = 0\n",
    "                # Save best model state\n",
    "                best_model_state = model.state_dict().copy()\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            # Stop if no improvement for 'patience' epochs\n",
    "            if patience_counter >= patience:\n",
    "                # Restore best model\n",
    "                if best_model_state is not None:\n",
    "                    model.load_state_dict(best_model_state)\n",
    "                break\n",
    "        \n",
    "        return best_val_acc, val_loss\n",
    "    \n",
    "    def train_with_cv(self, hidden_layers, learning_rate, batch_size, \n",
    "                     dropout_rate, epochs, weight_decay,\n",
    "                     patience=5, min_delta=0.0001):\n",
    "        \"\"\"\n",
    "        Train model with K-Fold Cross-Validation and early stopping\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        cv_scores : dict with mean and std of validation metrics\n",
    "        \"\"\"\n",
    "        # Use StratifiedKFold to maintain class distribution\n",
    "        skf = StratifiedKFold(n_splits=self.n_folds, shuffle=True, random_state=42)\n",
    "        \n",
    "        fold_accs = []\n",
    "        fold_losses = []\n",
    "        \n",
    "        # Convert to numpy for sklearn compatibility\n",
    "        y_train_np = self.y_train.cpu().numpy()\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(skf.split(self.X_train, y_train_np), 1):\n",
    "            # Create model for this fold\n",
    "            model = self.create_model(hidden_layers, dropout_rate)\n",
    "            \n",
    "            # Split data for this fold\n",
    "            train_X = self.X_train[train_idx]\n",
    "            train_y = self.y_train[train_idx]\n",
    "            val_X = self.X_train[val_idx]\n",
    "            val_y = self.y_train[val_idx]\n",
    "            \n",
    "            # Create data loader for training set\n",
    "            train_dataset = TensorDataset(train_X, train_y)\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            \n",
    "            # Train on this fold with early stopping\n",
    "            val_acc, val_loss = self.train_single_fold(\n",
    "                model, train_loader, val_X, val_y, \n",
    "                learning_rate, epochs, weight_decay,\n",
    "                patience, min_delta\n",
    "            )\n",
    "            \n",
    "            fold_accs.append(val_acc)\n",
    "            fold_losses.append(val_loss)\n",
    "        \n",
    "        # Aggregate results across folds\n",
    "        cv_scores = {\n",
    "            'mean_val_acc': np.mean(fold_accs),\n",
    "            'std_val_acc': np.std(fold_accs),\n",
    "            'mean_val_loss': np.mean(fold_losses),\n",
    "            'std_val_loss': np.std(fold_losses),\n",
    "            'fold_accs': fold_accs\n",
    "        }\n",
    "        \n",
    "        return cv_scores\n",
    "    \n",
    "    def train_model(self, model, learning_rate, batch_size, epochs, weight_decay=0,\n",
    "                   patience=10, min_delta=0.0001):\n",
    "        \"\"\"\n",
    "        Train a single model configuration with early stopping (no CV)\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        patience : int, default=10\n",
    "            Number of epochs to wait for improvement before stopping\n",
    "        min_delta : float, default=0.0001\n",
    "            Minimum change in test accuracy to qualify as improvement\n",
    "        \"\"\"\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_dataset = TensorDataset(self.X_train, self.y_train)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        # Early stopping variables\n",
    "        best_test_acc = 0.0\n",
    "        patience_counter = 0\n",
    "        best_model_state = None\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            for batch_X, batch_y in train_loader:\n",
    "                outputs = model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            # Evaluate on both train and test sets\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                # Train accuracy\n",
    "                train_outputs = model(self.X_train)\n",
    "                train_preds = torch.argmax(train_outputs, dim=1)\n",
    "                train_acc = (train_preds == self.y_train).float().mean().item()\n",
    "                \n",
    "                # Test accuracy\n",
    "                test_outputs = model(self.X_test)\n",
    "                test_preds = torch.argmax(test_outputs, dim=1)\n",
    "                test_acc = (test_preds == self.y_test).float().mean().item()\n",
    "            \n",
    "            # Early stopping check\n",
    "            if test_acc > best_test_acc + min_delta:\n",
    "                best_test_acc = test_acc\n",
    "                patience_counter = 0\n",
    "                # Save best model state\n",
    "                best_model_state = model.state_dict().copy()\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            # Stop if no improvement for 'patience' epochs\n",
    "            if patience_counter >= patience:\n",
    "                # Restore best model\n",
    "                if best_model_state is not None:\n",
    "                    model.load_state_dict(best_model_state)\n",
    "                break\n",
    "        \n",
    "        # Final evaluation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            train_outputs = model(self.X_train)\n",
    "            train_preds = torch.argmax(train_outputs, dim=1)\n",
    "            train_acc = (train_preds == self.y_train).float().mean().item()\n",
    "            \n",
    "            test_outputs = model(self.X_test)\n",
    "            test_preds = torch.argmax(test_outputs, dim=1)\n",
    "            test_acc = (test_preds == self.y_test).float().mean().item()\n",
    "        \n",
    "        return train_acc, test_acc\n",
    "    \n",
    "    def evaluate_configuration(self, params, epochs, patience=5, min_delta=0.0001):\n",
    "        \"\"\"\n",
    "        Evaluate a configuration with or without CV\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        result : dict with performance metrics\n",
    "        \"\"\"\n",
    "        if self.use_cv:\n",
    "            # Cross-validation\n",
    "            cv_scores = self.train_with_cv(\n",
    "                params['hidden_layers'],\n",
    "                params['learning_rate'],\n",
    "                params['batch_size'],\n",
    "                params['dropout_rate'],\n",
    "                epochs,\n",
    "                params['weight_decay'],\n",
    "                patience,\n",
    "                min_delta\n",
    "            )\n",
    "            \n",
    "            # Train final model on full training set to get test performance\n",
    "            model = self.create_model(params['hidden_layers'], params['dropout_rate'])\n",
    "            train_acc, test_acc = self.train_model(\n",
    "                model, params['learning_rate'], params['batch_size'],\n",
    "                epochs, params['weight_decay'], patience, min_delta\n",
    "            )\n",
    "            \n",
    "            result = {\n",
    "                **params,\n",
    "                'cv_mean_val_acc': cv_scores['mean_val_acc'],\n",
    "                'cv_std_val_acc': cv_scores['std_val_acc'],\n",
    "                'cv_fold_accs': cv_scores['fold_accs'],\n",
    "                'train_acc': train_acc,\n",
    "                'test_acc': test_acc,\n",
    "                'overfit_gap': train_acc - test_acc\n",
    "            }\n",
    "        else:\n",
    "            # Simple train/test split\n",
    "            model = self.create_model(params['hidden_layers'], params['dropout_rate'])\n",
    "            train_acc, test_acc = self.train_model(\n",
    "                model, params['learning_rate'], params['batch_size'],\n",
    "                epochs, params['weight_decay'], patience, min_delta\n",
    "            )\n",
    "            \n",
    "            result = {\n",
    "                **params,\n",
    "                'train_acc': train_acc,\n",
    "                'test_acc': test_acc,\n",
    "                'overfit_gap': train_acc - test_acc\n",
    "            }\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def grid_search(self, param_grid, epochs=50, patience=5, min_delta=0.0001):\n",
    "        \"\"\"\n",
    "        Perform grid search over hyperparameters with optional CV and early stopping\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        param_grid : dict\n",
    "            Dictionary with parameters names (str) as keys and lists of parameter\n",
    "            settings to try as values.\n",
    "            Example:\n",
    "            {\n",
    "                'hidden_layers': [[128], [128, 64], [256, 128, 64]],\n",
    "                'learning_rate': [0.001, 0.01, 0.1],\n",
    "                'batch_size': [32, 64, 128],\n",
    "                'dropout_rate': [0.0, 0.2, 0.5],\n",
    "                'weight_decay': [0, 1e-5, 1e-4]\n",
    "            }\n",
    "        epochs : int, default=50\n",
    "            Maximum number of training epochs\n",
    "        patience : int, default=5\n",
    "            Number of epochs to wait for improvement before early stopping\n",
    "        min_delta : float, default=0.0001\n",
    "            Minimum change to qualify as improvement\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        best_params : dict\n",
    "            Best parameters found\n",
    "        \"\"\"\n",
    "        cv_status = f\"with {self.n_folds}-fold CV\" if self.use_cv else \"without CV\"\n",
    "        print(f\"Starting Grid Search {cv_status} with early stopping (patience={patience})...\")\n",
    "        print(f\"Total combinations: {np.prod([len(v) for v in param_grid.values()])}\\n\")\n",
    "        \n",
    "        # Get all combinations\n",
    "        keys = param_grid.keys()\n",
    "        values = param_grid.values()\n",
    "        \n",
    "        for i, combination in enumerate(itertools.product(*values), 1):\n",
    "            params = dict(zip(keys, combination))\n",
    "            \n",
    "            print(f\"[{i}] Testing: {params}\")\n",
    "            \n",
    "            # Evaluate configuration\n",
    "            result = self.evaluate_configuration(params, epochs, patience, min_delta)\n",
    "            self.results.append(result)\n",
    "            \n",
    "            # Print results\n",
    "            if self.use_cv:\n",
    "                print(f\"   CV Val Acc: {result['cv_mean_val_acc']:.4f} ± {result['cv_std_val_acc']:.4f}\")\n",
    "                print(f\"   Test Acc: {result['test_acc']:.4f}\")\n",
    "                print(f\"   Fold Accs: {[f'{acc:.4f}' for acc in result['cv_fold_accs']]}\\n\")\n",
    "            else:\n",
    "                print(f\"   Train Acc: {result['train_acc']:.4f}, Test Acc: {result['test_acc']:.4f}\\n\")\n",
    "        \n",
    "        return self.get_best_params()\n",
    "    \n",
    "    def get_best_params(self, metric='test_acc'):\n",
    "        \"\"\"Get best parameters based on specified metric\"\"\"\n",
    "        if not self.results:\n",
    "            return None\n",
    "        \n",
    "        # For CV, use CV validation accuracy\n",
    "        if self.use_cv and 'cv_mean_val_acc' in self.results[0]:\n",
    "            metric = 'cv_mean_val_acc'\n",
    "        \n",
    "        best = max(self.results, key=lambda x: x[metric])\n",
    "        return best\n",
    "    \n",
    "    def print_results(self, top_n=5):\n",
    "        \"\"\"Print top N results\"\"\"\n",
    "        if not self.results:\n",
    "            print(\"No results to display. Run grid_search first.\")\n",
    "            return\n",
    "        \n",
    "        if self.use_cv and 'cv_mean_val_acc' in self.results[0]:\n",
    "            sorted_results = sorted(self.results, key=lambda x: x['cv_mean_val_acc'], reverse=True)\n",
    "            metric_name = 'CV Val Acc'\n",
    "        else:\n",
    "            sorted_results = sorted(self.results, key=lambda x: x['test_acc'], reverse=True)\n",
    "            metric_name = 'Test Acc'\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"TOP {top_n} CONFIGURATIONS (by {metric_name})\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        for i, result in enumerate(sorted_results[:top_n], 1):\n",
    "            print(f\"\\nRank {i}:\")\n",
    "            print(f\"  Architecture: {result['hidden_layers']}\")\n",
    "            print(f\"  Learning Rate: {result['learning_rate']}\")\n",
    "            print(f\"  Batch Size: {result['batch_size']}\")\n",
    "            print(f\"  Dropout: {result['dropout_rate']}\")\n",
    "            print(f\"  Weight Decay: {result['weight_decay']}\")\n",
    "            \n",
    "            if self.use_cv and 'cv_mean_val_acc' in result:\n",
    "                print(f\"  CV Val Acc: {result['cv_mean_val_acc']:.4f} ± {result['cv_std_val_acc']:.4f}\")\n",
    "                print(f\"  Fold Accuracies: {[f'{acc:.4f}' for acc in result['cv_fold_accs']]}\")\n",
    "            \n",
    "            print(f\"  Test Accuracy: {result['test_acc']:.4f}\")\n",
    "            print(f\"  Train Accuracy: {result['train_acc']:.4f}\")\n",
    "            print(f\"  Overfit Gap: {result['overfit_gap']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "example-usage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: (26048, 16)\n",
      "First 5 rows before transformation:\n",
      "   Unnamed: 0  age         workclass  fnlwgt     education  education-num  \\\n",
      "0        5514   33         Local-gov  198183     Bachelors             13   \n",
      "1       19777   36           Private   86459     Assoc-voc             11   \n",
      "2       10781   58  Self-emp-not-inc  203039           9th              5   \n",
      "3       32240   21           Private  180190     Assoc-voc             11   \n",
      "4        9876   27           Private  279872  Some-college             10   \n",
      "\n",
      "       marital-status       occupation   relationship   race     sex  \\\n",
      "0       Never-married   Prof-specialty  Not-in-family  White  Female   \n",
      "1  Married-civ-spouse  Exec-managerial        Husband  White    Male   \n",
      "2           Separated     Craft-repair  Not-in-family  White    Male   \n",
      "3  Married-civ-spouse  Farming-fishing        Husband  White    Male   \n",
      "4            Divorced    Other-service  Not-in-family  White    Male   \n",
      "\n",
      "   capital-gain  capital-loss  hours-per-week native-country income  \n",
      "0             0             0              50  United-States   >50K  \n",
      "1             0          1887              50  United-States   >50K  \n",
      "2             0             0              40  United-States  <=50K  \n",
      "3             0             0              46  United-States  <=50K  \n",
      "4             0             0              40  United-States  <=50K  \n",
      "==================================================\n",
      "\n",
      "Missing values by column:\n",
      "workclass         1447\n",
      "occupation        1454\n",
      "native_country     458\n",
      "dtype: int64\n",
      "\n",
      "Target encoding: {'<=50K': np.int64(0), '>50K': np.int64(1)}\n",
      "Target distribution: [0.75929054 0.24070946]\n",
      "\n",
      "Identified numeric features (9): ['age', 'fnlwgt', 'education_num', 'capital_gain', 'capital_loss', 'hours_per_week', 'capital_net', 'has_capital_gain', 'has_capital_loss']\n",
      "Identified categorical features (10): ['workclass', 'education', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'native_country', 'age_group', 'work_hours_category']\n",
      "\n",
      "Train set: (20838, 118), Test set: (5210, 118)\n",
      "Train target distribution: [0.75928592 0.24071408]\n",
      "Test target distribution: [0.75930902 0.24069098]\n",
      "\n",
      "Processed shapes - Train: torch.Size([20838, 118]), Test: torch.Size([5210, 118])\n",
      "Tensor shapes - Train: torch.Size([20838, 118]), Test: torch.Size([5210, 118])\n",
      "Tensor dtypes - X: torch.float32, y: torch.int64\n",
      "\n",
      "==================================================\n",
      "PREPROCESSING COMPLETED SUCCESSFULLY!\n",
      "==================================================\n",
      "Starting Grid Search with 5-fold CV with early stopping (patience=3)...\n",
      "Total combinations: 48\n",
      "\n",
      "[1] Testing: {'hidden_layers': [128], 'learning_rate': 0.001, 'batch_size': 32, 'dropout_rate': 0.0, 'weight_decay': 0}\n",
      "   CV Val Acc: 0.8614 ± 0.0050\n",
      "   Test Acc: 0.8493\n",
      "   Fold Accs: ['0.8587', '0.8541', '0.8611', '0.8637', '0.8692']\n",
      "\n",
      "[2] Testing: {'hidden_layers': [128], 'learning_rate': 0.001, 'batch_size': 32, 'dropout_rate': 0.0, 'weight_decay': 1e-05}\n",
      "   CV Val Acc: 0.8598 ± 0.0053\n",
      "   Test Acc: 0.8489\n",
      "   Fold Accs: ['0.8556', '0.8524', '0.8611', '0.8625', '0.8675']\n",
      "\n",
      "[3] Testing: {'hidden_layers': [128], 'learning_rate': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'weight_decay': 0}\n",
      "   CV Val Acc: 0.8605 ± 0.0057\n",
      "   Test Acc: 0.8493\n",
      "   Fold Accs: ['0.8589', '0.8529', '0.8589', '0.8613', '0.8704']\n",
      "\n",
      "[4] Testing: {'hidden_layers': [128], 'learning_rate': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'weight_decay': 1e-05}\n",
      "   CV Val Acc: 0.8611 ± 0.0042\n",
      "   Test Acc: 0.8509\n",
      "   Fold Accs: ['0.8568', '0.8575', '0.8606', '0.8623', '0.8685']\n",
      "\n",
      "[5] Testing: {'hidden_layers': [128], 'learning_rate': 0.001, 'batch_size': 64, 'dropout_rate': 0.0, 'weight_decay': 0}\n",
      "   CV Val Acc: 0.8610 ± 0.0047\n",
      "   Test Acc: 0.8518\n",
      "   Fold Accs: ['0.8599', '0.8546', '0.8587', '0.8630', '0.8687']\n",
      "\n",
      "[6] Testing: {'hidden_layers': [128], 'learning_rate': 0.001, 'batch_size': 64, 'dropout_rate': 0.0, 'weight_decay': 1e-05}\n",
      "   CV Val Acc: 0.8616 ± 0.0060\n",
      "   Test Acc: 0.8509\n",
      "   Fold Accs: ['0.8613', '0.8515', '0.8620', '0.8630', '0.8702']\n",
      "\n",
      "[7] Testing: {'hidden_layers': [128], 'learning_rate': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'weight_decay': 0}\n",
      "   CV Val Acc: 0.8605 ± 0.0062\n",
      "   Test Acc: 0.8489\n",
      "   Fold Accs: ['0.8558', '0.8520', '0.8606', '0.8649', '0.8695']\n",
      "\n",
      "[8] Testing: {'hidden_layers': [128], 'learning_rate': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'weight_decay': 1e-05}\n",
      "   CV Val Acc: 0.8616 ± 0.0053\n",
      "   Test Acc: 0.8526\n",
      "   Fold Accs: ['0.8572', '0.8556', '0.8623', '0.8618', '0.8709']\n",
      "\n",
      "[9] Testing: {'hidden_layers': [128], 'learning_rate': 0.01, 'batch_size': 32, 'dropout_rate': 0.0, 'weight_decay': 0}\n",
      "   CV Val Acc: 0.8577 ± 0.0059\n",
      "   Test Acc: 0.8451\n",
      "   Fold Accs: ['0.8553', '0.8505', '0.8575', '0.8565', '0.8685']\n",
      "\n",
      "[10] Testing: {'hidden_layers': [128], 'learning_rate': 0.01, 'batch_size': 32, 'dropout_rate': 0.0, 'weight_decay': 1e-05}\n",
      "   CV Val Acc: 0.8573 ± 0.0041\n",
      "   Test Acc: 0.8468\n",
      "   Fold Accs: ['0.8584', '0.8508', '0.8565', '0.8570', '0.8637']\n",
      "\n",
      "[11] Testing: {'hidden_layers': [128], 'learning_rate': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'weight_decay': 0}\n",
      "   CV Val Acc: 0.8590 ± 0.0048\n",
      "   Test Acc: 0.8432\n",
      "   Fold Accs: ['0.8553', '0.8524', '0.8589', '0.8623', '0.8661']\n",
      "\n",
      "[12] Testing: {'hidden_layers': [128], 'learning_rate': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'weight_decay': 1e-05}\n",
      "   CV Val Acc: 0.8576 ± 0.0053\n",
      "   Test Acc: 0.8514\n",
      "   Fold Accs: ['0.8544', '0.8527', '0.8556', '0.8577', '0.8678']\n",
      "\n",
      "[13] Testing: {'hidden_layers': [128], 'learning_rate': 0.01, 'batch_size': 64, 'dropout_rate': 0.0, 'weight_decay': 0}\n",
      "   CV Val Acc: 0.8591 ± 0.0048\n",
      "   Test Acc: 0.8528\n",
      "   Fold Accs: ['0.8568', '0.8544', '0.8580', '0.8579', '0.8683']\n",
      "\n",
      "[14] Testing: {'hidden_layers': [128], 'learning_rate': 0.01, 'batch_size': 64, 'dropout_rate': 0.0, 'weight_decay': 1e-05}\n",
      "   CV Val Acc: 0.8579 ± 0.0054\n",
      "   Test Acc: 0.8489\n",
      "   Fold Accs: ['0.8584', '0.8515', '0.8548', '0.8570', '0.8675']\n",
      "\n",
      "[15] Testing: {'hidden_layers': [128], 'learning_rate': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'weight_decay': 0}\n",
      "   CV Val Acc: 0.8587 ± 0.0050\n",
      "   Test Acc: 0.8480\n",
      "   Fold Accs: ['0.8589', '0.8500', '0.8587', '0.8603', '0.8656']\n",
      "\n",
      "[16] Testing: {'hidden_layers': [128], 'learning_rate': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'weight_decay': 1e-05}\n",
      "   CV Val Acc: 0.8583 ± 0.0050\n",
      "   Test Acc: 0.8493\n",
      "   Fold Accs: ['0.8570', '0.8512', '0.8563', '0.8611', '0.8661']\n",
      "\n",
      "[17] Testing: {'hidden_layers': [128, 64], 'learning_rate': 0.001, 'batch_size': 32, 'dropout_rate': 0.0, 'weight_decay': 0}\n",
      "   CV Val Acc: 0.8594 ± 0.0047\n",
      "   Test Acc: 0.8459\n",
      "   Fold Accs: ['0.8582', '0.8529', '0.8582', '0.8603', '0.8675']\n",
      "\n",
      "[18] Testing: {'hidden_layers': [128, 64], 'learning_rate': 0.001, 'batch_size': 32, 'dropout_rate': 0.0, 'weight_decay': 1e-05}\n",
      "   CV Val Acc: 0.8595 ± 0.0046\n",
      "   Test Acc: 0.8524\n",
      "   Fold Accs: ['0.8570', '0.8532', '0.8594', '0.8611', '0.8671']\n",
      "\n",
      "[19] Testing: {'hidden_layers': [128, 64], 'learning_rate': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'weight_decay': 0}\n",
      "   CV Val Acc: 0.8602 ± 0.0044\n",
      "   Test Acc: 0.8516\n",
      "   Fold Accs: ['0.8582', '0.8544', '0.8584', '0.8627', '0.8673']\n",
      "\n",
      "[20] Testing: {'hidden_layers': [128, 64], 'learning_rate': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'weight_decay': 1e-05}\n",
      "   CV Val Acc: 0.8594 ± 0.0052\n",
      "   Test Acc: 0.8509\n",
      "   Fold Accs: ['0.8587', '0.8512', '0.8596', '0.8599', '0.8675']\n",
      "\n",
      "[21] Testing: {'hidden_layers': [128, 64], 'learning_rate': 0.001, 'batch_size': 64, 'dropout_rate': 0.0, 'weight_decay': 0}\n",
      "   CV Val Acc: 0.8613 ± 0.0051\n",
      "   Test Acc: 0.8489\n",
      "   Fold Accs: ['0.8587', '0.8546', '0.8601', '0.8632', '0.8699']\n",
      "\n",
      "[22] Testing: {'hidden_layers': [128, 64], 'learning_rate': 0.001, 'batch_size': 64, 'dropout_rate': 0.0, 'weight_decay': 1e-05}\n",
      "   CV Val Acc: 0.8599 ± 0.0049\n",
      "   Test Acc: 0.8516\n",
      "   Fold Accs: ['0.8572', '0.8544', '0.8606', '0.8584', '0.8687']\n",
      "\n",
      "[23] Testing: {'hidden_layers': [128, 64], 'learning_rate': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'weight_decay': 0}\n",
      "   CV Val Acc: 0.8613 ± 0.0049\n",
      "   Test Acc: 0.8528\n",
      "   Fold Accs: ['0.8616', '0.8532', '0.8601', '0.8632', '0.8683']\n",
      "\n",
      "[24] Testing: {'hidden_layers': [128, 64], 'learning_rate': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'weight_decay': 1e-05}\n",
      "   CV Val Acc: 0.8615 ± 0.0046\n",
      "   Test Acc: 0.8514\n",
      "   Fold Accs: ['0.8606', '0.8553', '0.8584', '0.8651', '0.8680']\n",
      "\n",
      "[25] Testing: {'hidden_layers': [128, 64], 'learning_rate': 0.01, 'batch_size': 32, 'dropout_rate': 0.0, 'weight_decay': 0}\n",
      "   CV Val Acc: 0.8583 ± 0.0060\n",
      "   Test Acc: 0.8488\n",
      "   Fold Accs: ['0.8541', '0.8510', '0.8568', '0.8615', '0.8680']\n",
      "\n",
      "[26] Testing: {'hidden_layers': [128, 64], 'learning_rate': 0.01, 'batch_size': 32, 'dropout_rate': 0.0, 'weight_decay': 1e-05}\n",
      "   CV Val Acc: 0.8577 ± 0.0064\n",
      "   Test Acc: 0.8455\n",
      "   Fold Accs: ['0.8575', '0.8505', '0.8522', '0.8596', '0.8687']\n",
      "\n",
      "[27] Testing: {'hidden_layers': [128, 64], 'learning_rate': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'weight_decay': 0}\n",
      "   CV Val Acc: 0.8569 ± 0.0043\n",
      "   Test Acc: 0.8514\n",
      "   Fold Accs: ['0.8534', '0.8529', '0.8556', '0.8579', '0.8647']\n",
      "\n",
      "[28] Testing: {'hidden_layers': [128, 64], 'learning_rate': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'weight_decay': 1e-05}\n",
      "   CV Val Acc: 0.8575 ± 0.0034\n",
      "   Test Acc: 0.8434\n",
      "   Fold Accs: ['0.8589', '0.8524', '0.8556', '0.8579', '0.8627']\n",
      "\n",
      "[29] Testing: {'hidden_layers': [128, 64], 'learning_rate': 0.01, 'batch_size': 64, 'dropout_rate': 0.0, 'weight_decay': 0}\n",
      "   CV Val Acc: 0.8583 ± 0.0065\n",
      "   Test Acc: 0.8497\n",
      "   Fold Accs: ['0.8592', '0.8505', '0.8575', '0.8543', '0.8699']\n",
      "\n",
      "[30] Testing: {'hidden_layers': [128, 64], 'learning_rate': 0.01, 'batch_size': 64, 'dropout_rate': 0.0, 'weight_decay': 1e-05}\n",
      "   CV Val Acc: 0.8593 ± 0.0056\n",
      "   Test Acc: 0.8539\n",
      "   Fold Accs: ['0.8616', '0.8534', '0.8551', '0.8575', '0.8690']\n",
      "\n",
      "[31] Testing: {'hidden_layers': [128, 64], 'learning_rate': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'weight_decay': 0}\n",
      "   CV Val Acc: 0.8594 ± 0.0056\n",
      "   Test Acc: 0.8480\n",
      "   Fold Accs: ['0.8558', '0.8548', '0.8551', '0.8618', '0.8695']\n",
      "\n",
      "[32] Testing: {'hidden_layers': [128, 64], 'learning_rate': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'weight_decay': 1e-05}\n",
      "   CV Val Acc: 0.8580 ± 0.0039\n",
      "   Test Acc: 0.8472\n",
      "   Fold Accs: ['0.8580', '0.8524', '0.8560', '0.8596', '0.8642']\n",
      "\n",
      "[33] Testing: {'hidden_layers': [256, 128], 'learning_rate': 0.001, 'batch_size': 32, 'dropout_rate': 0.0, 'weight_decay': 0}\n",
      "   CV Val Acc: 0.8603 ± 0.0049\n",
      "   Test Acc: 0.8478\n",
      "   Fold Accs: ['0.8608', '0.8527', '0.8580', '0.8620', '0.8678']\n",
      "\n",
      "[34] Testing: {'hidden_layers': [256, 128], 'learning_rate': 0.001, 'batch_size': 32, 'dropout_rate': 0.0, 'weight_decay': 1e-05}\n",
      "   CV Val Acc: 0.8598 ± 0.0057\n",
      "   Test Acc: 0.8495\n",
      "   Fold Accs: ['0.8570', '0.8534', '0.8568', '0.8623', '0.8697']\n",
      "\n",
      "[35] Testing: {'hidden_layers': [256, 128], 'learning_rate': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'weight_decay': 0}\n",
      "   CV Val Acc: 0.8593 ± 0.0052\n",
      "   Test Acc: 0.8532\n",
      "   Fold Accs: ['0.8572', '0.8524', '0.8570', '0.8623', '0.8678']\n",
      "\n",
      "[36] Testing: {'hidden_layers': [256, 128], 'learning_rate': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'weight_decay': 1e-05}\n",
      "   CV Val Acc: 0.8612 ± 0.0047\n",
      "   Test Acc: 0.8522\n",
      "   Fold Accs: ['0.8599', '0.8553', '0.8594', '0.8615', '0.8697']\n",
      "\n",
      "[37] Testing: {'hidden_layers': [256, 128], 'learning_rate': 0.001, 'batch_size': 64, 'dropout_rate': 0.0, 'weight_decay': 0}\n",
      "   CV Val Acc: 0.8592 ± 0.0050\n",
      "   Test Acc: 0.8457\n",
      "   Fold Accs: ['0.8577', '0.8541', '0.8577', '0.8579', '0.8687']\n",
      "\n",
      "[38] Testing: {'hidden_layers': [256, 128], 'learning_rate': 0.001, 'batch_size': 64, 'dropout_rate': 0.0, 'weight_decay': 1e-05}\n",
      "   CV Val Acc: 0.8595 ± 0.0047\n",
      "   Test Acc: 0.8489\n",
      "   Fold Accs: ['0.8584', '0.8520', '0.8594', '0.8613', '0.8666']\n",
      "\n",
      "[39] Testing: {'hidden_layers': [256, 128], 'learning_rate': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'weight_decay': 0}\n",
      "   CV Val Acc: 0.8596 ± 0.0054\n",
      "   Test Acc: 0.8514\n",
      "   Fold Accs: ['0.8580', '0.8522', '0.8589', '0.8599', '0.8690']\n",
      "\n",
      "[40] Testing: {'hidden_layers': [256, 128], 'learning_rate': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'weight_decay': 1e-05}\n",
      "   CV Val Acc: 0.8614 ± 0.0053\n",
      "   Test Acc: 0.8530\n",
      "   Fold Accs: ['0.8620', '0.8522', '0.8601', '0.8649', '0.8678']\n",
      "\n",
      "[41] Testing: {'hidden_layers': [256, 128], 'learning_rate': 0.01, 'batch_size': 32, 'dropout_rate': 0.0, 'weight_decay': 0}\n",
      "   CV Val Acc: 0.8573 ± 0.0053\n",
      "   Test Acc: 0.8503\n",
      "   Fold Accs: ['0.8568', '0.8500', '0.8548', '0.8587', '0.8663']\n",
      "\n",
      "[42] Testing: {'hidden_layers': [256, 128], 'learning_rate': 0.01, 'batch_size': 32, 'dropout_rate': 0.0, 'weight_decay': 1e-05}\n",
      "   CV Val Acc: 0.8570 ± 0.0041\n",
      "   Test Acc: 0.8488\n",
      "   Fold Accs: ['0.8572', '0.8510', '0.8551', '0.8579', '0.8637']\n",
      "\n",
      "[43] Testing: {'hidden_layers': [256, 128], 'learning_rate': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'weight_decay': 0}\n",
      "   CV Val Acc: 0.8571 ± 0.0059\n",
      "   Test Acc: 0.8453\n",
      "   Fold Accs: ['0.8589', '0.8486', '0.8529', '0.8596', '0.8656']\n",
      "\n",
      "[44] Testing: {'hidden_layers': [256, 128], 'learning_rate': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'weight_decay': 1e-05}\n",
      "   CV Val Acc: 0.8587 ± 0.0049\n",
      "   Test Acc: 0.8505\n",
      "   Fold Accs: ['0.8565', '0.8520', '0.8570', '0.8618', '0.8663']\n",
      "\n",
      "[45] Testing: {'hidden_layers': [256, 128], 'learning_rate': 0.01, 'batch_size': 64, 'dropout_rate': 0.0, 'weight_decay': 0}\n",
      "   CV Val Acc: 0.8595 ± 0.0053\n",
      "   Test Acc: 0.8455\n",
      "   Fold Accs: ['0.8584', '0.8500', '0.8611', '0.8625', '0.8656']\n",
      "\n",
      "[46] Testing: {'hidden_layers': [256, 128], 'learning_rate': 0.01, 'batch_size': 64, 'dropout_rate': 0.0, 'weight_decay': 1e-05}\n",
      "   CV Val Acc: 0.8594 ± 0.0046\n",
      "   Test Acc: 0.8480\n",
      "   Fold Accs: ['0.8575', '0.8541', '0.8568', '0.8611', '0.8675']\n",
      "\n",
      "[47] Testing: {'hidden_layers': [256, 128], 'learning_rate': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'weight_decay': 0}\n",
      "   CV Val Acc: 0.8570 ± 0.0061\n",
      "   Test Acc: 0.8478\n",
      "   Fold Accs: ['0.8536', '0.8493', '0.8560', '0.8587', '0.8675']\n",
      "\n",
      "[48] Testing: {'hidden_layers': [256, 128], 'learning_rate': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'weight_decay': 1e-05}\n",
      "   CV Val Acc: 0.8574 ± 0.0055\n",
      "   Test Acc: 0.8353\n",
      "   Fold Accs: ['0.8541', '0.8498', '0.8568', '0.8603', '0.8659']\n",
      "\n",
      "\n",
      "================================================================================\n",
      "TOP 5 CONFIGURATIONS (by CV Val Acc)\n",
      "================================================================================\n",
      "\n",
      "Rank 1:\n",
      "  Architecture: [128]\n",
      "  Learning Rate: 0.001\n",
      "  Batch Size: 64\n",
      "  Dropout: 0.0\n",
      "  Weight Decay: 1e-05\n",
      "  CV Val Acc: 0.8616 ± 0.0060\n",
      "  Fold Accuracies: ['0.8613', '0.8515', '0.8620', '0.8630', '0.8702']\n",
      "  Test Accuracy: 0.8509\n",
      "  Train Accuracy: 0.8682\n",
      "  Overfit Gap: 0.0174\n",
      "\n",
      "Rank 2:\n",
      "  Architecture: [128]\n",
      "  Learning Rate: 0.001\n",
      "  Batch Size: 64\n",
      "  Dropout: 0.2\n",
      "  Weight Decay: 1e-05\n",
      "  CV Val Acc: 0.8616 ± 0.0053\n",
      "  Fold Accuracies: ['0.8572', '0.8556', '0.8623', '0.8618', '0.8709']\n",
      "  Test Accuracy: 0.8526\n",
      "  Train Accuracy: 0.8725\n",
      "  Overfit Gap: 0.0199\n",
      "\n",
      "Rank 3:\n",
      "  Architecture: [128, 64]\n",
      "  Learning Rate: 0.001\n",
      "  Batch Size: 64\n",
      "  Dropout: 0.2\n",
      "  Weight Decay: 1e-05\n",
      "  CV Val Acc: 0.8615 ± 0.0046\n",
      "  Fold Accuracies: ['0.8606', '0.8553', '0.8584', '0.8651', '0.8680']\n",
      "  Test Accuracy: 0.8514\n",
      "  Train Accuracy: 0.8706\n",
      "  Overfit Gap: 0.0192\n",
      "\n",
      "Rank 4:\n",
      "  Architecture: [256, 128]\n",
      "  Learning Rate: 0.001\n",
      "  Batch Size: 64\n",
      "  Dropout: 0.2\n",
      "  Weight Decay: 1e-05\n",
      "  CV Val Acc: 0.8614 ± 0.0053\n",
      "  Fold Accuracies: ['0.8620', '0.8522', '0.8601', '0.8649', '0.8678']\n",
      "  Test Accuracy: 0.8530\n",
      "  Train Accuracy: 0.8808\n",
      "  Overfit Gap: 0.0279\n",
      "\n",
      "Rank 5:\n",
      "  Architecture: [128]\n",
      "  Learning Rate: 0.001\n",
      "  Batch Size: 32\n",
      "  Dropout: 0.0\n",
      "  Weight Decay: 0\n",
      "  CV Val Acc: 0.8614 ± 0.0050\n",
      "  Fold Accuracies: ['0.8587', '0.8541', '0.8611', '0.8637', '0.8692']\n",
      "  Test Accuracy: 0.8493\n",
      "  Train Accuracy: 0.8678\n",
      "  Overfit Gap: 0.0185\n",
      "\n",
      "================================================================================\n",
      "TRAINING FINAL MODEL WITH BEST PARAMETERS\n",
      "================================================================================\n",
      "\n",
      "Final Model Performance:\n",
      "Architecture: [128]\n",
      "Train Accuracy: 0.8890\n",
      "Test Accuracy: 0.8482\n",
      "Overfit Gap: 0.0409\n",
      "\n",
      "Model saved to 'best_model.pth'\n"
     ]
    }
   ],
   "source": [
    "# Load data \n",
    "df = pd.read_csv(\"/Users/lawsonlevin/ML/project1/data/project_adult.csv\")\n",
    "\n",
    "# Preprocess data\n",
    "X_train, X_test, y_train, y_test, preprocessor, label_encoder = preprocess_data(df, return_preprocessor=True)\n",
    "\n",
    "# ============================================================\n",
    "# Grid Search with 5-Fold Cross-Validation and Early Stopping\n",
    "# ============================================================\n",
    "tuner_cv = MLPTuner(X_train, X_test, y_train, y_test, use_cv=True, n_folds=5)\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'hidden_layers': [[128], [128, 64], [256, 128]],\n",
    "    'learning_rate': [0.001, 0.01],\n",
    "    'batch_size': [32, 64],\n",
    "    'dropout_rate': [0.0, 0.2],\n",
    "    'weight_decay': [0, 1e-5]\n",
    "}\n",
    "\n",
    "# Run grid search with early stopping\n",
    "best_params = tuner_cv.grid_search(\n",
    "    param_grid, \n",
    "    epochs=50,       # Maximum epochs\n",
    "    patience=3,      # Stop if no improvement for 3 epochs\n",
    "    min_delta=0.0001 # Minimum improvement threshold\n",
    ")\n",
    "\n",
    "# Print top 5 configurations\n",
    "tuner_cv.print_results(top_n=5)\n",
    "\n",
    "# ============================================================\n",
    "# Train Final Model with Best Parameters\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING FINAL MODEL WITH BEST PARAMETERS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "final_model = tuner_cv.create_model(\n",
    "    best_params['hidden_layers'], \n",
    "    best_params['dropout_rate']\n",
    ")\n",
    "\n",
    "train_acc, test_acc = tuner_cv.train_model(\n",
    "    final_model,\n",
    "    best_params['learning_rate'],\n",
    "    best_params['batch_size'],\n",
    "    epochs=100,      # Train longer for final model\n",
    "    weight_decay=best_params['weight_decay'],\n",
    "    patience=10,     # More patience for final model\n",
    "    min_delta=0.0001\n",
    ")\n",
    "\n",
    "print(f\"\\nFinal Model Performance:\")\n",
    "print(f\"Architecture: {best_params['hidden_layers']}\")\n",
    "print(f\"Train Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Overfit Gap: {train_acc - test_acc:.4f}\")\n",
    "\n",
    "# Save the model\n",
    "torch.save(final_model.state_dict(), 'best_model.pth')\n",
    "print(\"\\nModel saved to 'best_model.pth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfca51c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Sequential:\n\tsize mismatch for 0.weight: copying a param with shape torch.Size([128, 118]) from checkpoint, the shape in current model is torch.Size([128, 116]).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     22\u001b[39m model = nn.Sequential(*layers)\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# 3. Load weights\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbest_model.pth\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m model.eval()\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ Model loaded!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ML/pyml-book/lib/python3.12/site-packages/torch/nn/modules/module.py:2624\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2616\u001b[39m         error_msgs.insert(\n\u001b[32m   2617\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m   2618\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2619\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[32m   2620\u001b[39m             ),\n\u001b[32m   2621\u001b[39m         )\n\u001b[32m   2623\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2624\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2625\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2626\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m   2627\u001b[39m         )\n\u001b[32m   2628\u001b[39m     )\n\u001b[32m   2629\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for Sequential:\n\tsize mismatch for 0.weight: copying a param with shape torch.Size([128, 118]) from checkpoint, the shape in current model is torch.Size([128, 116])."
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# LOAD MODEL AND MAKE PREDICTIONS\n",
    "# ============================================================\n",
    "\n",
    "# 1. Paste hyperparameters here:\n",
    "HIDDEN_LAYERS = [128]      \n",
    "DROPOUT_RATE = 0.0              \n",
    "INPUT_SIZE = 116              \n",
    "OUTPUT_SIZE = 2                 \n",
    "\n",
    "# 2. Recreate model architecture\n",
    "layers = []\n",
    "prev_size = INPUT_SIZE\n",
    "for hidden_size in HIDDEN_LAYERS:\n",
    "    layers.append(nn.Linear(prev_size, hidden_size))\n",
    "    layers.append(nn.ReLU())\n",
    "    if DROPOUT_RATE > 0:\n",
    "        layers.append(nn.Dropout(DROPOUT_RATE))\n",
    "    prev_size = hidden_size\n",
    "layers.append(nn.Linear(prev_size, OUTPUT_SIZE))\n",
    "\n",
    "model = nn.Sequential(*layers)\n",
    "\n",
    "# 3. Load weights\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "print(\"✅ Model loaded!\")\n",
    "\n",
    "# 4. Load and preprocess new data\n",
    "new_data = pd.read_csv(\"project1/data/project_validation_inputs.csv\")\n",
    "\n",
    "X_new = preprocess_data(new_data, has_target=False, preprocessor=preprocessor)\n",
    "\n",
    "# 5. Make predictions\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_new)\n",
    "    probabilities = torch.softmax(outputs, dim=1)\n",
    "    predicted_classes = torch.argmax(outputs, dim=1).numpy()\n",
    "\n",
    "# 6. Create results\n",
    "results = pd.DataFrame({\n",
    "    'predicted_class': predicted_classes,\n",
    "    'confidence': probabilities.max(dim=1)[0].numpy(),\n",
    "    'prob_class_0': probabilities[:, 0].numpy(),\n",
    "    'prob_class_1': probabilities[:, 1].numpy()\n",
    "})\n",
    "\n",
    "# Combine with original data\n",
    "results = pd.concat([new_data.reset_index(drop=True), results], axis=1)\n",
    "\n",
    "# Save\n",
    "results.to_csv('predictions.csv', index=False)\n",
    "\n",
    "print(f\"✅ Made {len(predicted_classes)} predictions!\")\n",
    "print(f\"   Saved to: predictions.csv\")\n",
    "print(f\"\\nFirst 5 predictions:\")\n",
    "print(results.head())\n",
    "\n",
    "# ============================================================\n",
    "# 7. CREATE SUBMISSION FILE (Required Format)\n",
    "# ============================================================\n",
    "\n",
    "# Decode predictions to labels\n",
    "predicted_labels = label_encoder.inverse_transform(predicted_classes)\n",
    "\n",
    "# Transform to required format: 1 if '>50K' else -1\n",
    "transformed_predictions = [1 if x == '>50K' else -1 for x in predicted_labels]\n",
    "\n",
    "# Create submission dataframe\n",
    "submission = pd.DataFrame({\n",
    "    'prediction': transformed_predictions\n",
    "})\n",
    "\n",
    "# Save with required filename\n",
    "submission.to_csv('Group_24_MLP_PredictedOutputs.csv', index=False)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"✅ SUBMISSION FILE CREATED!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Filename: Group_24_MLP_PredictedOutputs.csv\")\n",
    "print(f\"Total predictions: {len(transformed_predictions)}\")\n",
    "print(f\"Predicted >50K (1): {transformed_predictions.count(1)}\")\n",
    "print(f\"Predicted <=50K (-1): {transformed_predictions.count(-1)}\")\n",
    "print(f\"\\nFirst 10 predictions:\")\n",
    "print(submission.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyml-book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
